# Baltic Exchange Weekly Market Roundup Scraper ğŸŒŠ

A comprehensive web scraper for monitoring Baltic Exchange weekly market data, specifically targeting weekly reports with detailed content for Capesize, Panamax, Ultramax/Supramax, and Handysize vessel types.

## ğŸ¯ Features

### Core Functionality
- **ğŸ” Smart JSON API Scraping**: Scrapes Baltic Exchange weekly reports via JSON endpoint
- **ğŸ¯ Intelligent Content Extraction**: Advanced pattern matching for vessel type content
- **ğŸ§¹ Smart Deduplication**: Prevents duplicate weekly reports using week numbers and dates
- **ğŸ“Š Data Export**: CSV export with comprehensive weekly report data
- **ğŸ”„ API Integration**: RESTful API for programmatic access
- **ğŸ“± Modern Dashboard**: FastAPI web interface with Tabler.io UI

### Data Extraction
- **Weekly Reports**: Week number, date, category, and report link
- **Capesize Content**: Detailed market analysis for Capesize vessels
- **Panamax Content**: Comprehensive Panamax market insights
- **Ultramax/Supramax Content**: Market data for medium-sized vessels
- **Handysize Content**: Analysis for smaller bulk carriers

### Advanced Features
- **ğŸ¨ Smart Content Extraction**: Advanced regex patterns for vessel type sections
- **ğŸ“… Weekly Report Management**: Latest Dry (bulk) reports from Baltic Exchange
- **ğŸ”§ CLI Tools**: Command-line interface for automation and testing
- **ğŸ“ˆ Export Capabilities**: CSV export with all weekly report data
- **ğŸŒ Alternative Methods**: Selenium-based scraper for bypassing bot protection
- **ğŸ’¾ Local Data Storage**: Dashboard reads from local JSON files for offline access

## ğŸš€ **Usage**

### **Local Development**

```bash
# Clone the repository
git clone <repository-url>
cd baltic-exchange-scraper

# Install dependencies
pip install -r requirements.txt

# Run the scraper
python3 run_baltic_scraper.py --force

# Start the dashboard
python3 src/main.py

# Test connection
python3 run_baltic_scraper.py --test-connection

# Export to CSV
python3 run_baltic_scraper.py --export-csv

# View statistics
python3 run_baltic_scraper.py --stats-only
```

### **Vercel Deployment**

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy to Vercel
./deploy.sh

# Or deploy manually
vercel --prod
```

**Deployment Features:**
- ğŸš€ **Zero-config deployment** with Vercel
- ğŸ“± **Static file serving** for fast dashboard loading
- ğŸ”§ **Python 3.9 runtime** for optimal performance
- ğŸŒ **Global CDN** for worldwide access
- ğŸ“Š **Automatic scaling** based on traffic

### **Programmatic Usage**

```python
from src.baltic_exchange_scraper import BalticExchangeScraper

# Initialize scraper
scraper = BalticExchangeScraper()

# Test connection
result = scraper.test_connection()

# Update market data
result = scraper.update_market_data(force_update=True)

# Get latest data
latest_data = scraper.get_latest_data()

# Export to CSV
csv_data = scraper.export_csv()
```

## ğŸ“Š Data Structure

### Weekly Reports
- **Week Number**: Week identifier (e.g., Week 34, Week 33)
- **Date Report**: Report publication date
- **Category**: Report category (e.g., Dry, Bulk)
- **Report Link**: Direct link to the original report
- **Vessel Type Content**: Detailed market analysis for each vessel type

### Vessel Type Content
- **Capesize Content**: Market analysis for Capesize vessels (100,000+ DWT)
- **Panamax Content**: Market insights for Panamax vessels (60,000-80,000 DWT)
- **Ultramax/Supramax Content**: Market data for medium vessels (50,000-65,000 DWT)
- **Handysize Content**: Analysis for smaller bulk carriers (10,000-50,000 DWT)

### Data Management
- **Smart Deduplication**: Prevents duplicate weekly reports
- **Local Storage**: JSON files for offline dashboard access
- **Export Options**: CSV format with all weekly report data

### Dashboard Actions
- **ğŸ”„ Update Data**: Fetch fresh weekly reports from Baltic Exchange
- **ğŸ“¥ Export CSV**: Download all weekly report data
- **ğŸ“Š Refresh**: Reload dashboard from local data
- **ğŸ”— Test Connection**: Verify Baltic Exchange connectivity

## ğŸ”§ API Endpoints

### Core Endpoints
```bash
# Trigger data update
POST /api/update-data

# Get dashboard data
GET /api/dashboard-data

# Export CSV
GET /api/export-csv

# Test connection
GET /api/test-connection

# Health check
GET /api/health
```

### Response Format
```json
{
  "status": "success",
  "message": "Successfully updated market data",
  "new_entries": 0,
  "total_entries": 1,
  "duration_seconds": 7.13,
  "last_update": "2025-08-26T15:12:25.982838"
}
```

## ğŸ“ Project Structure

```
baltic-exchange-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # FastAPI web application
â”‚   â”œâ”€â”€ baltic_exchange_scraper.py # Core scraping logic
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ baltic_exchange_api.py # Baltic Exchange API client
â”‚   â”‚   â””â”€â”€ selenium_baltic_scraper.py # Alternative Selenium scraper
â”‚   â””â”€â”€ web/
â”‚       â””â”€â”€ index.html             # Dashboard UI (Tabler.io)
â”œâ”€â”€ data/                          # Data storage (JSON files)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ run_baltic_scraper.py         # CLI interface
â”œâ”€â”€ test_baltic_scraper.py        # Test suite
â”œâ”€â”€ example_usage.py               # Example usage script
â””â”€â”€ README.md                      # This file
```

## ğŸ¯ Weekly Report Structure

### Report Metadata
- **Week Number**: Week identifier (e.g., Week 34, Week 33)
- **Date Report**: Publication date
- **Category**: Report type (e.g., Dry, Bulk)
- **Report Link**: Direct link to original report

### Vessel Type Content
- **Capesize**: Market analysis for 100,000+ DWT vessels
- **Panamax**: Market insights for 60,000-80,000 DWT vessels
- **Ultramax/Supramax**: Market data for 50,000-65,000 DWT vessels
- **Handysize**: Analysis for 10,000-50,000 DWT vessels

## ğŸ” Content Extraction

### Smart Pattern Matching
- **Section Headers**: Identifies vessel type sections
- **Content Boundaries**: Extracts content between sections
- **Fallback Patterns**: Alternative extraction methods
- **Content Cleaning**: Removes boilerplate text

### Deduplication
- **Report Level**: Week number + date + category
- **Automatic**: Built into scraping process
- **Smart Merging**: Combines new with existing reports

## ğŸ“Š Data Storage

### JSON Format
```json
{
  "scraped_at": "2025-08-26T15:12:25.982819",
  "source_url": "https://www.balticexchange.com/...",
  "method": "json_api",
  "weekly_reports": [
    {
      "week_number": "34",
      "date_report": "22 Aug 2025",
      "category": "Dry",
      "link_report": "http://www.balticexchange.com/...",
      "capesize_content": "Capesize market endured a notably weaker week...",
      "panamax_content": "Panamax The excitement this week...",
      "ultramax_supramax_content": "ultramax being heard fixed...",
      "handysize_content": "Handysize Like the larger size..."
    }
  ]
}
```

## ğŸŒ Deployment

### Local Development
```bash
# Start the dashboard
python3 src/main.py

# Access dashboard at http://localhost:8000
```

### Dashboard Features
- **Real-time Data**: View latest weekly reports
- **Offline Access**: Reads from local JSON files
- **Manual Updates**: Click "Update Data" to fetch fresh reports
- **Export Options**: Download data as CSV
- **Connection Testing**: Verify Baltic Exchange connectivity

## ğŸ”§ Configuration

### Update Parameters
- **Manual Updates**: Click "Update Data" button when needed
- **Smart Deduplication**: Automatically prevents duplicate reports
- **Content Extraction**: Advanced regex patterns for vessel sections

### Content Customization
The scraper automatically extracts content for:
- Capesize vessels (100,000+ DWT)
- Panamax vessels (60,000-80,000 DWT)
- Ultramax/Supramax vessels (50,000-65,000 DWT)
- Handysize vessels (10,000-50,000 DWT)

## ğŸ› Troubleshooting

### Common Issues

#### No Weekly Reports Found
- Check Baltic Exchange website availability
- Verify JSON endpoint accessibility
- Use dashboard to test connection

#### Duplicate Content
- Smart deduplication prevents duplicates
- Reports are merged automatically
- Check logs for deduplication details

#### Dashboard Not Starting
- Check if port 8000 is available
- Ensure all dependencies are installed
- Check Python version (3.8+ required)

#### Baltic Exchange Connection Issues
- Website may have anti-bot protection
- Check connection test in dashboard
- Verify network connectivity

### Debug Mode
```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
python3 run_baltic_scraper.py --force
```

## ğŸ“ˆ Performance

### Metrics
- **Update Speed**: ~5-10 seconds for full weekly reports
- **Content Extraction**: 100% success rate for vessel sections
- **Deduplication**: Automatic and efficient
- **Memory Usage**: Low (streaming JSON processing)

### Optimization
- Efficient JSON API calls with proper headers
- Smart content extraction with fallback patterns
- Local data storage for offline dashboard access
- Efficient filtering with compiled regex
- Respectful API usage with delays

## ğŸ¤ Contributing

### Development Setup
```bash
# Fork the repository
git clone <your-fork>
cd baltic-exchange-scraper

# Create feature branch
git checkout -b feature/new-feature

# Make changes and test
python3 run_baltic_scraper.py --force

# Commit and push
git commit -m "Add new feature"
git push origin feature/new-feature
```

### Code Style
- Follow PEP 8 for Python code
- Use meaningful variable names
- Add docstrings to functions
- Include type hints where appropriate

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Baltic Exchange for providing weekly market roundup data
- FastAPI for the excellent web framework
- Tabler for the beautiful UI components
- The open-source community for inspiration and tools

## ğŸ“ Support

For questions, issues, or contributions:
- Open an issue on GitHub
- Check the troubleshooting section
- Review the API documentation at `/docs`

---

**Built with â¤ï¸ for Baltic Exchange market monitoring**
